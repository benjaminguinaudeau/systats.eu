---
layout: post 
title: "Predicting Political Affiliation from Text"
---

<br> <br>

This tutorial will cover the workflow and code of my shiny app \#PolTextPrediction. Some inspirations are directly taken from useR!2017. Lets start

<!--more-->
# Data Collection


The data comes from the [Manifesto Project](https://manifesto-project.wzb.eu/), who provide content analysis of partiesâ€™ electoral manifestos. It covers over 1000 parties from 1945 until today in over 50 countries on five continents. This is perfect news as we need training data that is pre-classified. This means the text
(*X*)
 should already be labeled
(*Y*)
. Thanks to the Manifesto Team there is a programmable access to their database via `manifestoR`. To download their data you simple have to set up an account and request a *API Key* (`mkey`) on their homepage on your perosnal profile (takes max 2 mins and is free). Copy the key into a string `" "` and login. In order to download data we must specify some arguments from `mp_corpus` to set the time range and regional filters. More options can be found in the [manifestoR vignette](https://github.com/ManifestoProject/manifestoR/blob/master/vignettes/manifestoRworkflow.Rmd).

``` r
# install.packages("manifestoR")
library(manifestoR)

mkey <- "c1d709849c34e15130f9052699c214af"
mp_setapikey(key = mkey)

m_corp <- mp_corpus(
    countryname %in% "Germany" & 
    edate > as.Date("2009-01-01")
  )

m_corp

library(tidytext)
# easy way of life.
m_data <- tidy(m_corp) 
```

The output object `m_corp` is from class `Corpus`, which is unneccessarily complex for doing data transformations on it. Therefore we use the `tidytext` package to get a clean and simple dataframe. They provide a very nice introduction into text mining in R. Free to access at ... . As we always want to reduce complexety lets drop some variables (by selecting few).

``` r
library(dplyr) # for %>% the pipe
names(m_data)

m_data <- m_data %>% 
  select(party, date, id, text)

glimpse(m_data)
```

Now we bring the data into its final shape. First the

``` r
# build look-up dataframe
party_data <- data.frame(
  old = c("41113", "41223", "41320", "41420", "41521", "41953"),
  new = c("The Left", "SPD", "The Greens", "FDP", "CDU/CSU", "AfD"),
  p_col = c("#8B1A1A", "#E2001A", "#46962b", "#ffed00", "black", "#1C86EE"),
  p_ord = 1:6,
  stringsAsFactors = F
)

# remove AfD because of only a few data points avaible
party_data <- party_data[-6, ] 

# this is untidy but robust. To be improved. 
m_data$y <- party_data$new[match(m_data$party, party_data$old)]
m_data$yord <- party_data$p_ord[match(m_data$party, party_data$old)]
cbind(m_data$y, m_data$party)

# remove unwished party
m_data <- m_data %>% 
  filter(!is.na(y))

# now transform character to factor and order factor by p_ord
# m_data$y <- factor(m_data$y, levels = party_data$new[order(party_data$p_ord)])
m_data$y <- as.factor(m_data$y)

# plot program frequency from 2009
library(ggplot2)
library(ggthemes) # beautiful

m_data %>%
  ggplot(aes(y, fill = y)) +
  geom_bar(alpha = .7) + 
  scale_fill_manual(values = party_data$p_col) +
  theme_gdocs() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme(legend.position = "none") +
  ggtitle("Availability of Electoral Programms per Party") +
  xlab("")
```

# Tokenization with `tidytext`


We will see how the undersampling of the AfD will affect the prediction accuracy. Now the decision has to be made how big are the text batches which the classifier is trained on. We try to imitate the user Shiny App input by assuming a average sentence size of 3. This is particularely short and has to be re-evaluated in the predicition flow. The task of breaking text into meaningful parts is called tokenization. As you can imagine there are several options avaible:

-   `ngrams`
-   `skip_ngrams`
-   `sentences`
-   `lines`
-   `paragraphs`
-   `regex`

``` r
library(tidytext)

# split every 50 words
m_tok <- m_data %>%
  unnest_tokens(
    output = sent, 
    input = text, 
    token = "sentences"
  ) 

m_tok <- m_tok %>%
  group_by(y) %>% # group by party
  mutate(seq = seq_along(sent) %/% 5) %>% # seq group of 3 sentences
  # mutate(seq = seq_along(words)%/%50) %>%
  group_by(y, seq) %>% # seq group & group by party
  summarise(batches = paste0(sent, collapse = " ")) # paste every 3 senetces

# m_tok$batches[2]

head(m_tok)
### implement functions
m_tok$n_char <- nchar(m_tok$batches)
m_tok$n_char
```

# Cleaning data


``` r
# devtools::update_packages("sy")
# devtools::install_github("systats/sy", force = T)
library(sy)
m_tok$fbatches <- sy::str_clean(m_tok$batches, 
                          to_lower = T, 
                          rm_punct = T, 
                          rm_stop = "de")
```

    ## Warning: package 'dplyr' was built under R version 3.4.1

    ## Warning: Installed Rcpp (0.12.12) different from Rcpp used to build dplyr (0.12.11).
    ## Please reinstall dplyr to avoid random crashes or undefined behavior.

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

# check data


``` r
head(m_tok)
```

    ## # A tibble: 6 x 5
    ## # Groups:   y [1]
    ##         y   seq
    ##    <fctr> <dbl>
    ## 1 CDU/CSU     0
    ## 2 CDU/CSU     1
    ## 3 CDU/CSU     2
    ## 4 CDU/CSU     3
    ## 5 CDU/CSU     4
    ## 6 CDU/CSU     5
    ## # ... with 3 more variables: batches <chr>, n_char <int>, fbatches <chr>

``` r
m_tok %>%
  group_by(y)%>%
  tally %>%
  ggplot(aes(y, n, fill = y)) + 
    geom_bar(stat = "identity")
```

![]({{ site.baseurl }}/images/2017-7-21-word2vec_files/figure-markdown_github/unnamed-chunk-8-1.png)

``` r
resampled <- m_tok %>% 
  filter(y %in% "The Left") %>%
  sample_frac(size = .6)

without <- m_tok %>% 
  filter(!(y %in% "The Left"))
  
balanced <- rbind(without, resampled)
m_tok <- balanced

m_tok %>%
  group_by(y)%>%
  tally %>%
  ggplot(aes(y, n, fill = y)) + 
    geom_bar(stat = "identity")
```

![]({{ site.baseurl }}/images/2017-7-21-word2vec_files/figure-markdown_github/unnamed-chunk-8-2.png)

# Training Test Data Split


``` r
set.seed(2017)

ids <- 1:nrow(m_tok)
train_ids <- sample(ids, .7*length(ids)) # 70 % train data ids
test_ids <- setdiff(ids, train_ids) # 30 % test data ids

train <- m_tok[train_ids,]
test <- m_tok[test_ids,]
```

# Neural Word Embeddings by `text2vec`

from words to numbers.

-   [NLP Stanford on Glove](https://nlp.stanford.edu/projects/glove/)
-   [text2vec on Glove](http://text2vec.org/glove.html)

``` r
# install.package("text2vec")
# devtools::install_github("dselivanov/text2vec")
library(text2vec)

# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(train$fbatches)

vocab <- create_vocabulary(it, ngram = c(2L, 2L)) %>%
  prune_vocabulary(term_count_min = 5L)

v_vect <- vocab_vectorizer(vocab)

# save(v_vect, file = "manifesto_multinom_vectorizer")
# load("manifesto_multinom_vectorizer")

train_dtm <- create_dtm(it, v_vect)
```

# Multiclass Logistic Regression with CV

``` r
# install.packages("doMC", force = T)
# install.packages("doMC", repos="http://R-Forge.R-project.org")
# devtools::install_github("rforge/domc")
library(doMC)
registerDoMC(cores = 4)

# install.packages("glmnet")
library(glmnet)
nfolds <- 4

t1 <- Sys.time()
cvfitmulti <- cv.glmnet(
    x = train_dtm,
    y = train[['y']],
    family = 'multinomial',
    type.multinomial = "grouped", 
    nfolds = nfolds,
    parallel = TRUE
)

print(difftime(Sys.time(), t1, units = 'sec'))

# performance checks
plot(cvfitmulti)

# save(cvfitmulti, file = "cvfitmulti")
# load("cvfitmulti")
```

``` r
text_vector <- function(x, text, vec){
  it <- itoken(x[[text]])
  dtm_test <- create_dtm(it, vectorizer = vec)
  dtm_test <- as.matrix(dtm_test)
  return(dtm_test)
}  

test_dtm <- text_vector(test,
                        text = "fbatches",
                        vec = v_vect)

preds <- predict(object = cvfitmulti, 
                 newx = data.matrix(test_dtm), 
                 type = 'class')

preds %>% head
train$y %>% head

library(caret)
#xtab <- table(preds, test$y)
confusionMatrix(preds, test$y)
```

``` r
library(flipMultivariates) # Our package containing the Random Forest routine
library(tm) # The package needed to convert the sparse matrix
df <- data.frame(y = train$y, as.matrix(train_dtm)) # Combine the outcome variable with the term document matrix
f <- formula(paste0("y ~ ", paste0(colnames(train_dtm), collapse = "+"))) # Create the R Formula which describes the relationship we are interrogating
library(randomForest)
rf <- randomForest(f, df) # Run the random forest model
rf
```
