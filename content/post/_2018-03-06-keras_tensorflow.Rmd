---
title: "Text Classification in R"
date: "2018-03-07"
categories: ["R", "ML", "NLP"]
tags: ["Keras", "TensorFlow", "Glove", "LSTM"]
description: "Building text classification models with keras and tensorflow trained on political party agendas (`manifestoR`)."
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, error = F)
```


This tutorial will cover how to perform text classification with `Keras` from R. This is a fairly new but incredible experience for R users who have been limited in the past to more high level machine learning libraries like `caret` and `h2o`. These packages are great for a variety of machine learning tasks. Despite some good feed forward networks, natural language processing or computer vision tasks require more complex network architectures to learn informative low dimensional feature representations. 


## Load Packages

If you want to reproduce this code you can get the corresponding github repo with all the code here. For more convenience the package management library `pacman` is used to (once install +) load the packages. Most of the packages used are part of the tidyverse.

```{r packs}
# devtools::install_github("rstudio/keras")
pacman::p_load(
  dplyr, stringr, manifestoR, purrr, keras, tidyr, 
  tidytext, tidyTX, keras, ggplot2, viridis, ggthemes
)
# keras::install_keras()
```


## Train & Test Set 

```{r}
load("data/pol_agendas.Rdata")
glimpse(pol_agendas)
```

For evaluation purpose the data is split into 90% train and 10% test set. 

```{r split}
set.seed(2018)
pol_agendas$split_id <- sample(1:2, size = nrow(pol_agendas), replace = T, prob=c(.9, .1))
train <- pol_agendas %>% filter(split_id == 1)
test <- pol_agendas %>% filter(split_id == 2)
```


# Keras

## Build Text Sequences

```{r prekeras2, echo = T, eval = T}
max_features <- 12000 # top most common words
batch_size <- 32
#maxlen <- 30 # Cut texts after this number of words (called earlier)

tokenizer <- text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, x = train$text_lemma)
#keras::save_text_tokenizer(tokenizer, "data/tokenizer")
#tokenizer <- keras::load_text_tokenizer("data/tokenizer")

train_seq <- tokenizer %>% 
  texts_to_sequences(train$text_lemma) %>% 
  pad_sequences(maxlen = maxlen, value = 0)

test_seq <- tokenizer %>% 
  texts_to_sequences(test$text_lemma) %>% 
  pad_sequences(maxlen = maxlen, value = 0)
```


```{r}
train[2,] %>%
  tidytext::unnest_tokens(words, text_word) %>%
  select(words) %>%
  cbind(., token = train_seq[2,]) %>% 
  t %>%
  as.data.frame %>%
  set_names(paste0("w", 1:30)) %>% 
  rownames_to_column(var = " ") %>%
  select(1:10) %>%
  knitr::kable()
```




## fasttext Model

Now letâ€™s define the models graph beginning with a embedding matrix. The embedding is a matrix with dimensions (vocabulary, embedding_size) acts as lookup table for the fixed size word vectors. 

```{r glove_fit}
glove_fit <- keras_model_sequential() %>%
  layer_embedding(
    input_dim = max_features, 
    output_dim = 128, 
    input_length = maxlen
    ) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(5, activation = "sigmoid") %>%
  compile(
    loss = "binary_crossentropy",
    optimizer = "adam",
    metrics = "accuracy"
  )

summary(glove_fit)
```

```{r, eval = F}
glove_hist <- glove_fit %>% 
  keras::fit(
    x = train_seq, 
    y = tx_onehot(train$party_id),
    batch_size = batch_size,
    epochs = 5, 
    validation_split = .2
  )
```



```{r, echo = F}
#keras::save_model_hdf5(glove_fit, "data/glove_fit.h5", include_optimizer = F, overwrite = T)
#save(glove_hist, file = "data/glove_hist")
glove_fit <- load_model_hdf5("data/glove_fit.h5")
#load("data/glove_hist")

# tx_keras_plot(glove_hist) +
#   theme_hc() +
#   scale_colour_manual(values = c("blue", "green"), labels = c("Train", "Validation"))
```


```{r pred1}
#plot(glove_hist)
preds_glove <- glove_fit %>%
  tx_keras_predict(test_seq, 1) %>% 
  as.vector()

caret::confusionMatrix(preds_glove, test$party_id)
tx_confusion(x = preds_glove, y = test$party_id, lib = "gg")
```


## LSTM model

```{r}
lstm_fit <- keras_model_sequential() %>%
  ### model arch
  layer_embedding(
    input_dim = max_features, 
    output_dim = 128, 
    input_length = maxlen
  ) %>% 
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>% 
  layer_dense(units = 5, activation = 'sigmoid') %>%
  ### compiler
  compile(
    loss = 'binary_crossentropy',
    optimizer = 'adam',
    metrics = c('accuracy')
  )

summary(lstm_fit)
```

```{r, eval = F}
lstm_hist <- lstm_fit %>% 
  keras::fit(
    x = train_seq, 
    y = tx_onehot(train$party_id),
    batch_size = batch_size,
    epochs = 3,
    validation_split = .2
  )
```


```{r, echo = F, eval = T}
#keras::save_model_hdf5(lstm_fit, "data/lstm_fit.h5", include_optimizer = F, overwrite = T)
lstm_fit <- keras::load_model_hdf5("data/lstm_fit.h5")
```


```{r fit2, eval = T}
# tx_keras_plot(lstm_hist)
preds_lstm <- lstm_fit %>% 
  tx_keras_predict(test_seq, 1)

caret::confusionMatrix(preds_lstm, test$party_id)
tx_confusion(x = preds_lstm, y = test$party_id, lib = "gg")
```


# Final Thoughts

* https://www.datacamp.com/community/tutorials/keras-r-deep-learning
* examples


